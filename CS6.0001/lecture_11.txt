Program efficiency part 2
  Reminder
    Can we estimate resources (typically time) to solve a problem
    Big O notation
    Started talking about certain algorithms

    Don't care about exact numbers, how does it grow
      A lot of interest is thinking in reverse
      How does an algorithm choice impact complexity?

    Don't care about time - affected by machine, language version etc.
    Focus on putting an upper bound on growth
      Worst case scenario
      But tight upper bound

  Today many more examples
    There are classes of complexit algorithms
    O(1) - constant running time - great
    O(log n) - logarithmic - very slow growith
    O(n) - linear
    O(n log n) - log-linear
    O(n^c) - polynomial
    O(c^n) - exponential

    Linear, log-linear not bad
    Beyond that gets painful

    Linear - growth from 10 to 100 same as growth 100 to 1000

  Now filling in the rest of the chart
    How you can begin to recognize where an algorithm lies

  Constant complexity
    Not many interesting algorithms here
    Can have a loop... if independent of input

  Logarithmic complexity
    Example much earlier: bisection search
    Another example
    Another trick called binary search
    Suppose I give you a list (numbers, integers)
      COULD just walk down the list - linear
    Suppose if sorted
      Again, could walk down the list, stop when bigger
        In the average case, faster
        Complexity is still linear
    How to do better
      Pick an index that divides the list in half
      Ask if L[i] == e
      If not, ask if L[i] is larger or smaller than e
      Depending on answer, search left or right half
      New version of divide and conquer
      Cut n to n/2 in one step
      So after i steps down to n/2^i
      Get down to 1 element, then stop
    What's the complexity of this
      Done when n/2^i = 1
        i = log(n) steps

    def bisect_search(L, e):
      if L == []: # Constant
        return False
      elif len(L) == 1: #Constant
        return L[0] == e
      else
        half = len(L)//2 # Constant
        if L[half] > e: # LOOKS constant... but it's not
          return bisect_search(L[:half], e)
        else:
          return bisect_search(L[half:], e)

    Why is it not constant? Actually copying list
    O(log n) search calls
    BUT to set up the search for each cell, need to copy list
    O(n) for each bisection search call to copy list
    O(log n) * O(n) -> O(n log n)
    Thinking carefully, not copying whole list
      But still doing order n work to do copying
    How do we fix this?
      Do I need to copy everything?
      Move pointers to middle of list


    def bisect_search(L, e):
      def bisect_search_helper(L, e, low, high):
        if high == low:
          return L[low] == e
        mid = (low + high)//2
        if L[mid] == e:
          return True
        elif L[mid] > e:
          if low == mid:
            return False
          else:
            return bisect_search_helper(L, e, low, mid - 1)
        else:
          return bisect_search_helper(L, e, mid + 1, high)
      if len(L) == 0:
        return False
      else:
        return bisect_search_helper(L, e, 0, len(L) - 1)

    Now we're down to O(log n)
    Cost at each point is constant
    This is a really nice example of a log complexity function
    Cheating slightly though
      Usually don't care about implementation
      But here implementation DOES impact complexity - so be careful!
    Think about the reduction in problem size each time
      Reduce by 1
        Linear
      Reduce by half, thirds, quarters
        Log (unless hidden cost somewhere)

  Another log example - converting integer to string
  Could just call str on it, but how to do inside machine

  def intToStr(i):
    digits = '0123456789'
    if i == 0:
      return '0'
    result = ''
    while i > 0:
      result = digits[i%10] + result
      i = i//10
    return result

  How many times do you go through loop?
    Dividing by 10? Log of(i)
      Number of digits in integer, not size of integer
      O(log(i))

  We've got constant, log, what about linear?
    Most examples last time were linear with iterative loops

  def fact_iter(n):
    prod = 1
    for i in range(1, n+1)
      prod *= i
    return prod

  Overall O(n)

  What about recursive?

  def fact_recur(n):
    if n <= 1:
      return 1
    else:
      return n * fact_recur(n - 1)

  If you time it, may run a bit slower than fact_iter due to function calls
  But still O(n) because number of function calls is linear with n
    Effort to set up call is constant

  Log-linear
    We're going to see this next time
    Merge sort!

  Polynomial complexity
    Saw this last time - nested loops, recursive function calls

  Exponential
    These are things we want to stay away from, sometimes can't
    Recursive function more than 1 recursive call
      Towers of Hanoi
      Move stack of n - 1 onto spare peg
      Move 1 to target
      Move remaining stack to target

    Let tn denote time to solve tower of size n

    tn = 2t(n-1) + 1
       = 2(2t(n-2) + 1) + 1
       = 4t(n-2 + 1) + 2 + 1
       = 8t(n-3) + 4 + 2 + 1

    After k steps
      a = 2^(n-1) + 2^(n-2)... + 4 + 2 + 1
      2a = 2^n + 2^(n-1)...
      a = 2^n
      2^n - 1
    Again characteristic step
      2 recursive calls

  Cool problem from math
    Powerset
    Assume I have a set of integers with no repeats
    Want to generate collection of all possible subsets
      up to n elemts
    {1, 2, 3} -> {}, {1}, {2}, {3}, {4}, {1,2}, {1,3}, {2,3}, {1, 2, 3}
    Order does not matter
    Handy problem in number or set theory
    You can imagine writing a big iterative loop
      A bunch of nested loops
    Nice recursive solution
      Let's assume we can solve a smaller size problem
    If can generate n, then can generate n - 1
      Take those, add n to each of those subset

    def genSubsets(L):
      if len(L) == 0: # constant
        return [[]]
      smaller = genSubsets(L:-1) # call n times - linear
      extra = L[-1:]
      new = []
      for small in smaller: # depends on how big smaller is... grows as a factor of 2
        new.append(small + extra)
      return smaller + new

    Nice crisp piece of code
    For a set of size k there are 2^k cases
      So overall complexity
      Here it's how the problem size grows each time around

    Complexity classes again
      O(1) - code does not depend on size of problem
      O(log n) - reduce problem in half each time
      O(n) - simple iterative or recursive program
      O(n log n) - will see next time
      O(n^c) - nested loops or recursive calls
      O(c^n) - multiple recursive calls at each level

    One more example
      Fibonacci

      def fib_iter(n):
        if n == 0:
          return 0
        elif n == 1:
          return 1
        else
          fib_i = 0
          fib_ii = 1
          for i in range(n-1):
            tmp = fib_i
            fib_i = fib_ii
            fib_ii = tmp + fib_ii
          return fib_ii

        Complexity is linear, O(n)

      def fib_recr(n):
        if n == 0:
          return 0
        elif n == 1:
          return 1
        else
          return fib_recur(n - 1) + fib_recur(n - 2)

      Pretty code
      But you've now got two recursive calls
      This is going to be exponential (2^n)
      But fib isn't balanced - tree doesn't go all the way down
      Does that change the complexity?
        Changes the base
        It's the golden ratio to the nth power
        Very cool proof (copious spare time!)

    When you're given a new problem - how do I get it linear if I can?
      Mostly stay away from exponential, but can't always

    Lists
      O(1) - index, store, length, append
      O(n) - ==, remove, copy, reverse, iteration, in list

    Dicts
      worst case
        O(n) - index, store, length, delete, iteration
      average case
        O(1) - index, store, delete, iteration

    Dictionaries give more power/flexibility, but comes with a cost
  Next: sorting

