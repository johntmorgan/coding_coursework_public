Expectation
Reading
  "Expectation" = "Expected Value"
  Expectation of a random variable -> mean or average
    More precisely when each value weighted according to probability
    Formally
      Ex[R] ::= sum lomega in S R(lomega) * Pr[lomega]
  EV of uniform random variable
    R the value from rolling die
    EV = 1 * 1/6 + 2 * 1/6 ... + 6 * 1/6 = 7/2
    Shows that random variable may never actually take on the value
    In general if Rn is a random variable with a uniform distribution
      Ex[Rn] = (a1 + a2 + ... an) / n
  EV of reciprocal random variable
    S = 1/R
    Ex[S] = Ex[1/R] = 1/1 * 1/6 + 1/2 * 1/6 ... + 1/6 * 1/6 = 49/120
    Note that Ex[1/R] != 1/Ex[R]
  EV of indicator random variable
    Just the probability of the event
    Ex[IA] = Pr[A]
    Example, if A is coin with bias p coming up heads
      Ex[IA] = Pr[IA = 1] = p
  Alternate definition
    Ex[R] = sum x in range(R) of x * Pr[R = x]
    Proof
      Ex[R] ::= sum lomega in S of R(lomega) * Pr[lomega]
              = sum x in range(R) sum lomega[R = x] R(lomega) * Pr[lomega]
              = sum x in range(R) sum lomega[R = x] x * Pr[lomega]
              = sum x in range(R) of x * sum lomega[R = x] Pr[lomega]
              = sum x in range(R) of x * Pr[R = x]
  Conditional Expectation
    Expectations can be conditioned on some event A
    Given random variable R, EV of R conditioned on A is the prob-weighted average of R over outcomes in A
    Formally
      Ex[R | A] ::= sum r in range(R) r * Pr[R = r | A]
    Example, roll die, given number at least 4
      Ex[R | R >= 4] = sum i=1 to 6 of i * Pr[R = i | i >= 4] = 1 * 0 + 2 * 0 + 3 * 0 + 4 * 1/3 + 5 * 1/3 + 6 * 1/3 = 5
    Important in dividing complicated expectation calcs into simpler cases
      Can calc in each simple case and then average, weighting by probability
    Example
      49.6% of people are male, 50.4% female
      male expected height 5'11"
      female expected height 5'5"
      What is the expected height of a randomly chosen person?
      Ex[H] = Ex[H | M] * Pr[M] + Ex[H | F] * Pr[F]
            = (5 + 11/12) * 0.496 + (5 + 5/12) * (1 - 0.496)
            = 5.6646... (Just a lil under 5'8")
    Theorem: Law of Total Expectation
      Let R be a random variable on a sample space S, and suppose A1, A2, ..., is a partition of S
      Ex[R] = sum i of Ex[R | Ai] * Pr[Ai]
  Mean Time to Failure
    Computer program crashes at the end of each hour with probability p, if has not crashed
    What is the expected time until the program crashes?
    A event that system fails on first step, NOT(A) system does not fail
    Ex[C] = Ex[C | A] * Pr[A] + Ex[C | NOT(A)] * Pr[NOT(A)]
    Ex[C | A] = 1
    Ex[C | NOT(A)] = 1 + Ex[C]
    Ex[C] = 1 * p + (1 + Ex[C])(1 - p)
          = p + 1 - p + (1 - p) * Ex[C]
          = 1 + (1 - p) * Ex[C]
          1 = Ex[C] - (1 - p) * Ex[C] = p * Ex[C]
    Ex[C] = 1/p
    General principle worth remembering
      If system independently fails at each time step with probability p
      The expected number of steps up to the first failure is 1/p
    So if there is a 1% chance crashes at end of each hour
      Expected time to crash = 1/0.01 = 100 hours
  Example
    Couple insists on having children until boy
    How many baby girls should they expect first
      Crash = having boy
      p = 1/2
      Expect boy after 1/p = 2 children
    So even in societies with this culture, population will divide evenly between boys and girls
      (Assuming no infanticide, ahem - JM)
  Intuitive argument for result
    Suppose system is restarted afer each failure
    Mean time to failure same as mean time between successive repeated failures
    Probability of failre at step is p
    After n steps, pn failures
    Average number of steps between failures = np / p = 1 / p
  Formal version
    Random variable like C that counts steps to first failure
    "Geometric distribution" with parameter p
    Defintion
      Random variable C has a geometric distribution with paramter p iff codomain C = Z+ and
      Pr[C = i] = (1 - p)^(i - 1) * p
    Lemma
      If random variable C has a geometric distribution with param p
      Ex[C] = 1/p
  Expected returns in gambling
    Win w dollars with probability p
    Lose x dollars with probability 1-p
    Expected return = p * w - (1-p) * l dollars
    Flip a coin, win $1 for heads lose $1 for tails
      1 / 2 * 1 - (1 - 1 / 2) * 1 = 0
      This game is "fair" - expected return is zero
  Splitting the pot
    A game that looks fair on first analysis (but isn't)
    3 players, each player puts $2 on the bar
    Write "heads" or "tails" secretly
    Flip a fair coin
    $6 on bar is "split" among correct predictions (or returned to all if all wrong)
    Four-step method
      Expected return is 0 when drawn out in tree
    Collusion
      Say opponents agree to write the opposite number
      You can never win the $4 - at most you win $1
      Ex[payoff] = 0 * 0 + 1 * 1/4 + 1 * 1/4 + 4 * 0 + -2 * 0 + -2 * 1/4 + -2 * 1/4 + 0 * 0
                 = -1/2
    How to win the lottery
      Many other opportunities to collude
      Weekly football betting pool - pick most games right, split large pot
      But if two playes collude by guessing differently, unfair advantage
      Lottery: MIT prof Chernoff
        Could make money - even though state takes large share before payout
        Discovered small set of numbers selected by large fraction
          Even if picking Red Sox winning average, the day's date
        As if players colluding to lose
          If any win, have to split the pot
        By selecting uniformly at random, would get whole pot for winning
          Win an average of 7c on the dollar
          So return +.07, not -.50
        (How did he find out the numbers others played? - JM)

  Linearity of Expectation
    EV obey a simple, very helpful rule
    EV of sum of random variables is the sum of the EVs of the variables
    Thm
      For any random variables R1 and R2
      Ex[R1 + R2] = Ex[R1] + Ex[R2]
      Proof
        Ex[T] = sum lomega in S of T(lomega) * Pr[lomega]
              = sum lomega in S of (R1(lomega) + R2(lomega)) * Pr(lomega)
              = sum lomega in S of R1(lomega) * Pr[lomega] + sum lomega in S of R2(lomega) * Pr[lomega]
              = Ex[R1] + Ex[R2]
    Thm
      For random variables R1, R2 and constants a1, a2 in Real
        Ex[a1 * R1 + a2 * R2] = a1 * Ex[R1] + a2 * Ex[R2]
      In other words, expectation is a linear function
    Routine induction extends to more than 2 vars
    Corollary (Linearity of Expectation)
      For any random variables R1, ..., Rk and constants a1, ...ak in Real
      Ex[sum i = 1 to k of ai * Ri] = sum i = 1 to k of ai * Ex[Ri]
    The great thing is no independence is required
    Example
      EV of sum of 2 fair dice
      R1 is the number on the first die
      R2 is the number on the second die
      Ex[R1 + R2] = Ex[R1] + Ex[R2] = 3.5 + 3.5 = 7
      Assuming dice independent, could use a tree diagram to prove this
        But would be a pain - 36 cases
      Without assuming independence, hard to apply tree
      But here you do not have to assume they are independent
        Expected sum = 7 as long as each is individually fair, even if linked together somehow
    Sums of Indicator Random Variables
      Example
        Dinner party where n men check hats
        Hats get mixed up during dinner, each gets a random one back
        Each gets own hat with probability 1/n
        Expected number of men (G) that get their own hat back
        Let Gi be an indicator for example each man gets his own hat
        G = G1 + G2 + ... + Gn
        Ex[Gi] = Pr[Gi = 1] = 1/n
        Ex[G] = Ex[G1 + G2 + ... + Gn]
              = Ex[G1] + Ex[G2] + ... + Ex[Gn]
              = 1/n + 1/n + 1/n... (n times)
              = 1
    More generally, provides a very good method for computing expected number of events
      Given any collection of events A1, A2, ..., An
      Expected number of events that will occur is
        Sum i = 1 to n of Pr[Ai]
    Expectation of a Binomial Distribution
      Flip n biased coins, each with probability p of coming up heads
      What is expected number of heads
      Let J be the random variable denoting the number of heads
      J has a binomial distributon with parameters n, p and
      Pr[J = k] = (n choose k) * p^k * (1 - p)^(n - k)
      Ex[J] = sum from k = 0 to n of k * Pr[J = k]
            = sum from k = 0 to n of k * (n choose k) * p^k * (1 - p)^(n - k)
      Looks rough but linearity leads to easy derivation of simple closed form
      Express J as a sum of indicator variables
        Let Ji be the indicator variable for the ith coin coming up heads
        Ji ::= 1 if the ith coin is heads
             = 0 if the ith coin is tails
      Number of heads J = J1 + J2 + ... + Jn
      Ex[J] = sum i = 1 to n of Pr[Ji] = pn
      So EV of binomial distribution with parameters n and p is simply pn
      What if coins are not mutually independent? Again, doesn't matter
    Coupon Collector problem
      Collecting n cars by buying kids meals
      Expected number of purchases to acquire at least one of each car
      Sequence, partitioning by ending whenever get a new car
      There are n colors collecting
      Let Xk be the length of the kth segment
      Total number of kids meals
      T = X0 + X1 + ... + X(n - 1)
      Let focus attention on Xk, length of kth segment
      Beginning of segment k, we have k different types of car
      Segment ends when acquire a new type
      Each kids meal contains a type we already have with probability k/n
      Therefore new meal with probability 1 - k / n = (n - k) / n
      Expected number of meals to get a new car = n /(n - k) (from mean time to failure rule)
      Now use linearity of expectation
      Ex[T] = Ex[X0 + X1 + ... + X(n - 1)]
            = n / (n - 0) + n / (n - 1) + ... n / 3 + n / 2 + n / 1
            = n * (1 / n + 1 / (n - 1) + ... + 1 / 3 + 1 / 2 + 1 / 1)
            = n * (1 / 1 + 1 / 2 + 1 / 3 + ... 1 / (n - 1) + 1 / n)
            = n * Hn (harmonic number!)
            ~nln(n)
      Expected number of dice rolls to see every number 1 to 6:
        6H6 = 14.7...
      Expected number of people to poll to cover every birthday:
        365H365 = 2364.6...
    Infinite Sums
      Also works for infinite number of random variables
        Provided the variables satsify absolute convergence criterion
      Thm
        Let R0, R1, ..., be random variables such that
        sum i = 0 to inf of Ex[|Ri|] converges. Then
          Ex[sum i = 0 to inf of Ri] = sum i = 0 to inf of Ex[Ri]
    A gambling paradox
      Can bet red or black at roulette
        House favored due to even money payout and 1-2 green slots (EU, US respectively)
        Even without slots, expected win = 0
      Bet doubling strategy
        Double every time you lose
        Red will eventually come up with probability 1, and you win!
        Something must be wrong
      Argument claiming expection is zero against a fair wheel flawed
        Implicit, invalid use of linearity of expectation for an infinite sum
      Let Bn be the number of $$ won on nth bet
      Bn = zero if red has already come up
      Dollar amount won
        Sum n = 1 to inf of Bn
      Expected win
        Ex[sum n = 1 to inf of Bn]
      Assuming wheel fair
        Ex[Bn] = 0 so
        Sum n = 1 to inf of Ex[Bn] = Sum n = 1 to inf of 0 = 0
      Flaw in argument you can't win is the implicit appeal to linearity of expectation
        Conclude the expectation equals the sum of expectations
        This is a case where linearity of expectation fails to hold
        Even though expectation is 10 and sum of expectations converges
      Problem is expecation of sum of absolute values of bets diverges
        So condition required for infinite linearity fails
        In particular, under bet doubling
        Your nth bet is 10 * 2^(n - 1) dollars
        Probability of making bet is 2^(-n)
        So Ex[|Bn|] = 10 * 2^(n - 1) * 2(-n) = 20
        Therefore sum n = 1 to inf of Ex[|Bn|] = 20 + 20 + 20
          Diverges rapidly
        So the presumption you can't beat a fair game is mistaken
        But probability theory shouldn't be rejected due to absurd conclusion
        If you only have a finite amount of money to bet with
        Say enough money to make k bets before going bakrupt
        Then your expectation is back to 0 for a fair wheel
        So absurd to assume you could follow a bet doubling strategy forever
        Absurd assumption -> absurd conclusion
    Expectations of Products
      Expectation of sum is sum of expectations
      This is NOT usually true for products
      Say we roll a die with result = random variable R
      Does Ex[R * R] = Ex[R] * Ex[R]?
      Ex[R] = 3.5
      Ex[R]^2 = 12.25
      Ex[R^2] = sum lomega in S R^2(lomega) * Pr[lomega]
              = sum i = 1 to 6 of i^2 * Pr[Ri = i]
              = 1^2 / 6 + 2^2 /6 + ... + 6^6 / 6 = 15 1/6 != 12 1/4
      So
        Ex[R * R] != Ex[R] * Ex[R]
      However this does hold when the random variables in the product are independent
      Thm
        For any two independent random variables R1, R2
        Ex[R1 * R2] = Ex[R1] * Ex[R2]
      Corollary - Expectation of Independent Product
        If random variables R1, R2, ..., Rk are mutually independent, then
          Ex[bpro i = 1 to k of Ri] = bpro i = 1 to k of Ex[Ri]

Lecture
  Great Expectations
  Abstraction - expectation of random variable
  Gambling example
    Carnival dice
    Choose a number from 1 to 6
    Roll 3 fair dice
    Payoff as follows
      $1 for every match
      Lose $1 if no match
    Example, choose 5
      Roll 2, 3, 4: lose $1
      Roll 5, 4, 6: win $1
      Roll 5, 4, 5: win $2
      Roll 5, 5, 5: win $3
      (Real CD: may only win $1, but this is a more generous version)
    Pr[0 fives] = (5 / 6)^3 = 125 / 216
    Pr[1 five] = 3 ch 1 * (5 / 6)^2 * 1 / 6 = 75 / 216
    Pr[2 fives] = 3 ch 1 * 5 / 6 * (1 / 6)^2 = 15 / 216
    Pr[3 fives] = (1/6 ^ 3) = 1 / 216
    What do you expect to win in 216 games?
    0 matches 125 times
    1 match 75 times
    2 matches 15 times
    3 matches 0 times
    So on average
    - 17 / 216 = -8c per game - NOT fair!
    Summarize
      You can "expect" to lose 8 cents per play
      But you never actually lose 8 cents on any single play
      This is just your *average* loss
  Expected Value
    Expected value of random variable R is the average value of R
      With values weighted by their probabilities
    E[R] ::= sum v * Pr[R = v]
      Assuming a countable sample space
    So E[$wint in carnival] = -17/216
  Alternative definition
    E[R] = sum lomega in S R(lomega) * Pr[lomega]
      Form helpful in some proofs, don't use it much in applications
      Boring derivation but you need to be able to do it
    Proof of equivalence:
      [R = v] ::= {lomega | R(lomega) = v] so
      Pr[R = v] ::= sum lomega in [R = v] of Pr[lomega]
    Now back to original of expectation of R
      E[R] ::= sum v in range(R) of v * Pr[R = v]
                                      * sum lomega in [R = v] of Pr[lomega]
             = sum v sum lomega in [R = v] of v * Pr[lomega]
             = sum v sum lomega in [R = v] of R(lomega) * Pr[lomega]
             = sum lomega in S of R(lomega) * Pr[lomega]
    Notice some top-level technical things now
      This proof, like many in basic probability theory
      Involves taking sums and rearranging terms in sums a lot
      Get away with sums instead of integrals because the sample space assumed to be countable
      S = {lomega0, lomega1, lomega2, lomega3, ... lomegan,..}
    Safe to rearrange terms
      Implicitly assuming the defining sum for the expectation is absolutely convergent
      Absolute convergence: sum of absolute values converge
    Absolute convergence
      Order not even specified here
      E[R] ::= sum v in range(R) of v * Pr[R = v]
      Terms on right could be rearranged to equaly anything at all when sum is not absolutely convergent
      When dealing with non-absolute value sums, rearranging is a no-no
      All reasoning in probability theory would be inapplicable
  Expected Value
    Also called "mean value" or "mean" or "expectation"
    Expectations & Averages
      From a pile of graded exams, pick one at random (uniformly) and let S be its score
      Outcomes are of uniform probability, while S may not be
      Now E[S] = the average exam score
        May be less informative than median or mean
      We can estimate averages
        by estimating expectations of random variables based on picking random elements
        "sampling"
    Impossible for all exams to be above average
      Pr[R > E[R]] < 1
    On the other hand
      Pr[R > E[R]] >= 1 - ee
        is possible for all ee > 0
        So can get as close to 1 as you want
        For example, almost everyone has an above average number of fingers
          Amputation is more common the polydactylism
  Expected number of Heads
    In n coinflips with bias p for Heads
    E[# Heads] = E[Bn,p]
               = sum k = 0 to n of k * (n choose k) * p^k * (1 - p)^(n - k)
               = sum k = 0 to n of k * (n choose k) * p^k * q^(n - k)
    Binomial theorem & differentiation gives closed formula
      (x + y)^n = sum k = 0 to n of (n choose k) * x^k * y^(n - k)
    Take d/dx
      n(x + y)^(n - 1) = 1/x * sum k = 0 to n of k * (n choose k) * x^k * y^(n - k)
      n(p + q)^(n - 1) = 1/p * sum k = 0 to n of k * (n choose k) * p^k * q^(n - k)
        p + q = 1
      n = 1/p * E[Bn,p]
      E[Bn,p] = np
  Total Expectation
    Like Total Probability
    Def. conditional expectation
    E[R | A] ::= sum of v * Pr[R = v | A]
    E[R] = E[R | A] * Pr[A] + E[R | NOT(A)] * Pr[NOT(A)]
    Generalizes to many cases
    E[R] = E[R1 | A1] * Pr[A] + E[R2 | A2] * Pr[A]...
    Where {Ai} partitions sample space
    (Can be an infinite sum)
    Expected # heads again
      Let e(n) ::= expected number H's in n flips
                 = 1 + e(n - 1) if first flip H
                 = e(n - 1) if first flip T
      By total expectation
      e(n) = [1 + e(n - 1)] * p + e(n - 1) * q
           = e(n - 1) + p - very simple recursive definition of e(n)
           = e(n - 2) + 2p
           = 0 + np
  Expected time to failure
    How many times pressed before a button fails?
    Part that ARM auto mechanic is waiting for comes in
    Let's formalize in terms of (possibly biased) coin flipping
    Head = failure, tail = success
    Pr[Head] = p
    F ::= # flips to first head
    E[F]?
    Pr[F = 1] = Pr[H] = p
    Pr[F = 2] = Pr[TH] = q * p
    Pr[F = 3] = Pr[TTH] = q^2 * p
    PDFF(n) = q^(n - 1) * p
      Geometric distribution - comes up all the time
    E[F] = sum n > 0 of n * Pr[F = n]
         = sum n > 0 n * q^(n - 1) p
         = p * sum n >= 0 (n + 1) * q^n - familiar generating function (?? - JM)
         = 1 / (1 - q)^2
         = 1 * 1 / p^2 = 1 / p
    So with a fair coin
      Expected number of flips to head = 1 / (1/2) = 2
    Let's derive this in a more intuition-friendly way (no generating functions or series)
      Look at outcome tree
      Call tree B, subtree is a copy of B
      Use total expectation
    E[F] = E[F | 1st is H] * p + E[F | 1st is T] * q
         = 1 * p + (1 + E[F]) * q
         = p * (1 + E[F])q
         = 1 / p
    Silly example for fun
      Space station Mir has 1/150000 chance of destruction in any given hour
      How many hours expected until destruction?
        150k hours = 17 years
  Intuitive argument (not made rigorous)
    E[#fails in one try] = p
    E[#fails in n tries] = np
    E[#tries between fails] = #tries/#fails = n / np = 1 / p
  Linearity of Expectations
    Makes things really easy to calculate
    R, S random variables and a, b constants
    E[aR + bS] = a * E[R] + b * E[S] - fundamental formula, remember this
    (Can extend to many variables with convergence, but let's focus on 2 for now)
    Proof by rearranging terms in the defining sum
      E[aA + bB] = sum lomega of (aA(lomega) + bB(lomega)) * Pr[lomega] =
      = a(sum lomega of A(w) * Pr[lomega]) + b(sum lomega of B(w) * Pr[lomega])
      = aE[A] + bE[B]
  Expectation of indicator IA
    E[IA] ::= 1 * Pr[IA = 1] + 0 * Pr[IA = 0]
            = Pr[IA = i]
            = Pr[A] // about to use this multiple times
  Expected # heads in n flips
    Hi is indicator for Head on ith flip
    #Hs = H1 + H2 + ... + Hn
    E[#Hs] = E[H1 + H2 + ... + Hn]
           = E[H1] + E[H2] + ... E[Hn]
           = n * Pr[Head] = np
  Expected # hats returned
    n men each check their hat at a hat check
    Hats get scrambled
    Pr[ith man gets own hat back] = 1/n
    How many men get their own hats back
    Ri indicates ith man got his hat returned
    Notice Ri and Rj are *not* independent!
      If Ri got his hat back, Rj's odds improve
    E[# hats returned] = E[R1 + R2 + ... + Rn]
                       = E[R1] + E[R2] + ... + E[Rn]
                       = n(1 /n) = 1
  Chinese Banquet
    Table of 9 in a circle, lazy Susan spins around
    Say n people sitting around a spinner with n different dishes
    Spin randomly
    How many people do we expect will get same dish as initially?
    Ri indicates ith person got the same dish
    Ris are *totally* dependent - all 1 or all 0
      All get same dish or don't
    But linearity still holds
    Expectation is still 1
  Independent Product of Expectations
    Requires independence
    E[X * Y] = E[X] * E[Y]
    Extends to many if mutually independent
    But do NOT assume without independence
    Example
      Say X takes positive and negative values with equal probability
      E[X] = 0 < E[X^2]
      E[X] * E[X] = 0
      Completely dependent - same random variable
      But illustrates failure of product rule
  Blunder
    Don't assume a reciprocal expectation rule
    E[X / Y] = E[X] / E[Y]
    Not true even when independent!
    Prominent people have made this blunder
